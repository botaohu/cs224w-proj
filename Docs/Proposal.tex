\documentclass[11pt,letterpaper]{article}

\newtheorem{theorem}{Theorem}
\newtheorem{corollary}{Corollary}
\newtheorem{lemma}{Lemma} 
\newtheorem{claim}{Claim}
\newtheorem{fact}{Fact}
\newtheorem{definition}{Definition}
\newtheorem{assumption}{Assumption}
\newtheorem{observation}{Observation}
\newtheorem{example}{Example}

\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{enumerate}
\usepackage[margin=0.75in]{geometry}

\oddsidemargin 0in
\evensidemargin 0in
\textwidth 6.5in
\topmargin -0.5in
\textheight 9.0in
\abovecaptionskip 0in

\def\@maketitle
   {
   \newpage
   \null
   \vskip .375in
   \begin{center}
      {\Large \bf \@title \par}
      % additional two empty lines at the end of the title
      \vspace*{24pt}
      {
      \large
      \lineskip .5em
      \begin{tabular}[t]{c}
         \ifcvprfinal\@author\else Anonymous CVPR submission\\
         \vspace*{1pt}\\%This space will need to be here in the final copy, so don't squeeze it out for the review copy.
Paper ID \cvprPaperID \fi
      \end{tabular}
      \par
      }
      % additional small space at the end of the author name
      \vskip .5em
      % additional empty line at the end of the title block
      \vspace*{12pt}
   \end{center}
   }

\def\abstract
   {%
   \centerline{\large\bf Abstract}%
   \vspace*{12pt}%
   \it%
   }

\def\endabstract
   {
   % additional empty line at the end of the abstract
   \vspace*{12pt}
   }


\begin{document}
%%%%%%%%% TITLE
\title{{CS224W Course Project Proposal \\} \bf De-anonymizing social networks via network alignment}

\author{Danqi Chen\\
Stanford University\\
{\tt\small danqi@stanford.edu}
% For a paper whose authors are all at the same institution,
% omit the following lines up until the closing ``}''.
% Additional authors and addresses can be added with ``\and'',
% just like the second author.
% To save space, use either the email address or home page, not both
\and
Botao Hu\\
Stanford University\\
{\tt\small botaohu@stanford.edu}
%
\and
Shuo Xie\\
Stanford University\\
{\tt\small shuoxie@stanford.edu}
}

\maketitle
\thispagestyle{empty}

\maketitle



\subsection{Problem Formulation}
Formally, the problem of de-anonymizing social networks is to identify the same users between two anonymized social networks.  A social network can be modeled as a directed graph (e.g., Twitter, Flickr) or an undirected graph (e.g., Facebook): Each node in the graph represents a user in the social network and each edge between two nodes characterizes the relationship between the users, such as ``following'' in Twitter or ``friendship'' in Facebook. In addition, anonymized graphs are usually released with at least some attributes in their nodes such as user profile information, user interests, corresponding to a set of attributes for each node in $V$, which may help us making de-anonymization easier.

\section{Literature Review}

\subsection{De-anonymizing Social Networks}

\cite{Narayanan2009} is the first work to perform large-scale de-anonymization of real-world social networks. Specifically, they try to match the users between Titter and Flickr and show that $30.8\%$ of the verifiable members of both social networks could be recognized with $12\%$ error rate. In their paper, they propose a greedy-based de-anonymization algorithm, which consists of two steps - ``seed finding'' and ``propagation''. In the ``seed finding'' step, they identify a small number of ``seed'' nodes which are present in the both networks and map them to each other; Later they use the seed nodes as ``anchors'' to propagate the de-anonymization to more and more nodes. They iteratively try to extend the mapping as follows: pick an arbitrary unmapped node in a graph, and map it to the ``most similar'' node in the other graph, where the similarity score is computed based on some heuristics such as cosine similarity or the number of formed squares.

It is worth noting that \cite{Narayanan2011} won the Kaggle Social Network Challenge by applying the above de-anonymizing algorithm to the contest dataset and real Flickr network, and utilze the connectivity among ``matched users'' in Flickr to perform link predictions. 

Although the proposed algorithm demonstrates the feasibility of successful de-anonymization on large-scale social networks, the algorithm is based on simple greedy approaches, so that the result is far from ``perfect matchings''; Furthermore, it is purely based on the network topology and doesn't incorporate any attribute of nodes, and these public attributes may greatly improve the de-anonymization process.

\subsection{Network Alignment}
Network alignment is another well-formed problem, which aims to perform matchings or alignments between the vertices of two undirected graphs. This problem has been studied extensively, and has been applied successfully in many domains such as finding common pathways in biological networks(\cite{Singh2007, Singh2008, Liao2009}), ontology matchings between the Library of Congress with the categories from Wikipedia(\cite{Bayati2009}), and multi-lingual ontologies(\cite{Kreitmann2011, Bayati2009a}). In this subsection, we will review the existing algorithms that produce good solutions for the network alignment, analyze the strengths and weakness of these algorithms and finally investigate the possibility to apply the network alignment algorithms on our settings of de-anonymizing social networks.
 
Most existing work formulate the network alignment problem as follows: given two sets of vertices $V_A = \{1, 2, \ldots, n\}, V_B = \{1', 2', \ldots, m'\}$, and $A = (V_A, E_A), B = (V_B, E_B)$ are two undirected graphs with their respective vertex and edge sets. Let $L$ be a bipartite graph between the vertices of $A$ and $B$, formally $L = (V_A \cup V_B, E_L)$ (in many algorithm settings, $L$ is the complete bipartite graph). The goal is to find a matching $M$ between $V_A$ and $V_B$ using only edges from $L$ ($M \subseteq L$), such that the number of overlapped edges is maximized. Here an edge $(i, j) \in E_A$ is \textit{overlapped} iff. $(i', j') \in E_B, (i, i')$ and $(j, j')$ belong to $M$.

\begin{center}
\includegraphics[scale = 0.4]{fig/matching}
\end{center}

In a more general setting following \cite{Singh2007}, each edge $e \in E_L$ has a non-negative weight $w_e$, and the goal is to find a matching $M$ maximizing a linear combination of the matching weight and the number of over-lapped edges. Mathematically, it can be formulated as a quadratic program(QP): define $\mathbf{S}$ as a 0-1 matrix of size $|E_L| \times |E_L|$ where $S[ii',  jj'] = 1$ if $(i, j) \in E_A$ and $(i', j') \in E_B$, $\mathbf{A}$ as the binary incidence matrix of $L$ with dimensions $|V_L| \times |E_L|$. We want to find a 0-1 vector $\mathbf{x}$ such that:
\begin{eqnarray*}
      \max_{\mathbf{x}} & & {\alpha \mathbf{w}^{\intercal}\mathbf{x} + \beta \mathbf{x}^{\intercal}\mathbf{S}\mathbf{x}} \\
      \text{ subject to  } & &  \mathbf{A}\mathbf{x} \leq \mathbf{1}, x_{ii'} \in \{0, 1\}
\end{eqnarray*}

Solving the QP formulation is NP-hard, therefore many existing works have been attempted to relax the constraints or find the heuristics:

\textbf{IsoRank}(\cite{Singh2008}) learns a similarity measure $r_{ii'}$ between node $i$ in $V_A$ and node $i'$ in $V_B$ and $r_{ii'}$ is defined as:
   \begin{equation*}
           r_{ii'} = \sum_{(i, j) \in E_A}{\sum_{(i', j') \in E_B}{\frac{R_{j, j'}}{|N_j||N_{j'}|}}}.
   \end{equation*}
The new weights $r_{ii'}$ can be found using an eigen-value calculation as in PageRank. Later, \cite{Bayati2009a} proposes an extension of IsoRank algorithm based on the Kronecker product, named \textbf{SpaIsoRank}, which can be adapted to the case when $L$ is sparse. \cite{Kollias2011} proposes a \textbf{Network Similarity Decomposition (NSD)} algorithm, which uses matrix decomposition to approximate the power iteration in IsoRank and runs much faster than the original algorithm.

\cite{Klau2009} formulates a \textbf{linear programming(LP)} relaxation of the original QP, and proposes an iterative approximation algorithm, which is a tigher LP relaxation and Lagrangian decomposition of the symmetry constraints. This algorithm can be used to handle large networks when $L$ is sparse.

\cite{Bayati2009a} proposes two message passing algorithms \textbf{MP} and \textbf{MP++} for network alignment: It constructs a fractor graph and defines a probability distribution on the space of all matchings in $L$ that assigns the highest probability to the matching that maximize the QP, and then applies the stanford Belief Progapation algorithm for finding the optimum solution. As well, MP algorithms can be applied to large sparse networks.

In last year's course project \cite{Kreitmann2011}, Kreitmann proposes a simple \textbf{simulated annealing(SA)} algorithm, which initializes a greedy-based matching first, and adopts simulated algorithm approach taking the number of ``overlapped'' edges as the heuristic function for swapping two matches. 

Although the above algorithms may produce good results in their applications, we still have some concerns:
\begin{itemize}
   \item
      To our knowledge, no network alignment algorithm has been applied to the problem of de-anonymizing social network.
   \item
      IsoRank, LP, and MP algorithms can only be applied in small datasets (e.g., biological networks) or large-scale sparse networks. For handling large-scale networks, how to select $L$ should be the issue we need to consider. 
   \item
      All the network alignment algorithms are based only on the network structure, no node attribute is incorporated to improve the alignment. 
   \item
      All the above algorithms are applying to aligning two undirected networks, we should consider if it is possible to develop network alignment algorithms for two directed network or one undirected network and one directed network, which can be adopted to our social networks. 
\end{itemize}

\section{Data and Evaluation}

We will use the data from three large online social networks in our experiments: Twitter, Flickr and Foursquare. On these social networks, the data of user profiles and friendship connections are all public and accessable by crawlers or APIs. 

The first graph is the ``following'' relationships on the Twitter\footnote{http://www.twitter.com}, a microblogging service, which has 500 million users (200 million active). We consider to adopt the data crawled by  Kwak et al. \footnote{http://an.kaist.ac.kr/traces/WWW2010.html} containing 41 million users. In order to increase the overlap to the other two social networks, we will extend this dataset to the latest network as possible. 

The second graph is the ``contact'' relationship on Flickr\footnote{http://www.flickr.com}, a photo-sharing service, which has 51 million registered members and 6 billion images on Jan 19, 2012.

The third graph is the ``Friends'' relationships on Foursquare\footnote{http://www.foursquare.com}, a location-based social network, which has 22 million global users on March 2, 2012. 

Narayanan at el. \cite{Narayanan2008} did the experiment on aligning Twitter and Flickr data.

\subsection{Ground truth}
To verify our de-anonymizing results , we have to determine the ground truth, i.e., the true mapping between the users of the online social networks. Actually, we do not need to label the mapping of all users since the ground truth as a test set can be far smaller than the complete network data.
Instead of labeling the user mapping by human editors, there are several sources to get the ground truth. 

\subsubsection{Single-source ground truth}

About.me\footnote{http://about.me} is a personal web hosting service, which had at least 1 million users on October, 2011 \footnote{http://techcrunch.com/2011/10/17/about-mes-ceo-on-how-to-hit-a-million-users-in-300-days-figure-out-who-your-entourage-is/}. The site offers registered users a simple platform from which to link multiple online identities, relevant external sites, and popular social networking websites such as Google+, Twitter, Facebook, LinkedIn, Flickr, YouTube, Foursquare. These links on user profile is naturally human-labelled mapping by the user itself, which can be seen as a zero-error ground truth.  We picked a random sample of the mappings and verified by human inspection that the most of about.me users have Twitter accounts and at least one of Flickr and Foursquare accounts. About.me also provides simple APIs to list user directory and view the links on user profile without the strict crawling limitation. Therefore, we will mainly adopt the data from about.me to be our ground truth in this project.

\subsubsection{Inferred multiple-source Ground truth}

The links of the user profile page of the social networking websites are another great sources for ground truth, which is also generated by the user itself. Usually, a single user has many accounts for different social networking website. On the user profile page, there might be links to this user's accounts in the other popular social networking webiste. Especially, nowadays, for the most the social networking website, the user logs in with the connection to his/her Twitter or Facebook account, and that website may show the user's Twitter and Facebook account in the user profile. For example, the figure \ref{fig:infer} shows how the links connects to other social networking website on the user profile page among the famous large social networking website: LinkedIn publicly shows the users' linked Twitter account and Gmail/Google+ account; and public Google+ profile reveals the user's Facebook and Twitter account; and Foursquare will show the user's login Twitter or Facebook account information. 

\begin{figure}[h!]
\centering
\caption{Links on the user profile page of serveral social networking website}
\label{fig:infer}
\includegraphics[width=0.7\textwidth]{infer.eps}
\end{figure}


Fortunately, on these famous social networking webiste in the figure \ref{fig:infer}, the most of user's profile pages are publicly accessbile. A crawler can easily follow these links on the profile page, discover all linked accounts about one user, and even retrieve the user's real name and affiliation from the profile on the real-name social networking website, such as LinkedIn and Facebook (colorred in orange and red in figure \ref{fig:infer}. Thus, we can build a ground truth by exploring all linked accounts of each user. 

\subsection{Evaluation}

We will compare our algorithm on the real dataset to Network Alignment \cite{Narayanan2008} and Simulated Annealling \cite{Kreitmann2011}. 

Accurary: Given a ground truth, the accurary evaluation can be simply the correct matches between two networks. 

Scalability: Running time on the large-scale data.

\section{Deliverable}
We will implement codes in SNAP\footnote{http://snap.stanford.edu} framework and integrate the complete componment of network alignment into the SNAP package.  


\nocite{Ding2010,Wanga,Peng2012,Klau2009,Wondracek2010,Balduzzi2010,Koutra2011,Bayati2009,Bradde2010,Cromi2009,Flannick2009,Memisevic2012,Kreitmann2011,Narayanan2009,Delcher2002,Kollias2012,Mohammadi,Kuchaiev2007,Wangb,Liao2009,El-Kebir2011,Shi,Bayati2009a,Pache2012,Pache2012a,Kollias2011,Doan,Bayatia,Koyuturk2006,Todor2007,Narayanan2008,Burkhart2010,Backstrom2007}


\bibliographystyle{abbrv}
\bibliography{na}

\section{Appendix}

\end{document}
